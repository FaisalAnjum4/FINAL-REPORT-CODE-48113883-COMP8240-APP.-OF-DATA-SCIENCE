{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaisalAnjum4/FINAL-REPORT-CODE-48113883-COMP8240-APP.-OF-DATA-SCIENCE/blob/main/Md_Faisal_Anjum_48113883_Final_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvBJuvqzs0gT",
        "outputId": "2e3e4df8-3323-4a8a-edea-73ccc9d72edd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8BO7D90s34n",
        "outputId": "e63c37cd-af13-402c-cea3-f16ebe0fbcce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296186 sha256=6dac6775615d97a94aa23346414cdc169f29f13af7a445f585ae5393090fd8fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REPLICATION OF YELP F DATASET**"
      ],
      "metadata": {
        "id": "2dx2TE1EmUE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_df = pd.read_csv('yelpf_train.csv')\n",
        "test_df = pd.read_csv('yelpf_test.csv')\n",
        "\n",
        "# Display the first few rows to understand the structure\n",
        "#print(\"Train Data Sample:\")\n",
        "#print(train_df.head())\n",
        "\n",
        "#print(\"\\nTest Data Sample:\")\n",
        "#print(test_df.head())\n",
        "\n",
        "# Rename columns to 'label' and 'text' (adjust column names if needed)\n",
        "train_df.columns = ['label', 'text']\n",
        "test_df.columns = ['label', 'text']\n",
        "\n",
        "# Clean the text: convert to lowercase, remove special characters, and strip whitespace\n",
        "train_df['text'] = train_df['text'].str.lower().str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True).str.strip()\n",
        "test_df['text'] = test_df['text'].str.lower().str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True).str.strip()\n",
        "\n",
        "# Prepare the training data for FastText\n",
        "train_df['fasttext_format'] = train_df['label'].apply(lambda x: f'__label__{x}') + ' ' + train_df['text']\n",
        "preprocessed_train_file = 'yelpf_train_fasttext.txt'\n",
        "train_df['fasttext_format'].to_csv(preprocessed_train_file, index=False, header=False)\n",
        "\n",
        "# Prepare the test data for FastText\n",
        "test_df['fasttext_format'] = test_df['label'].apply(lambda x: f'__label__{x}') + ' ' + test_df['text']\n",
        "preprocessed_test_file = 'yelpf_test_fasttext.txt'\n",
        "test_df['fasttext_format'].to_csv(preprocessed_test_file, index=False, header=False)\n",
        "\n",
        "print(\"\\nPreprocessing complete. Data saved for FastText.\")\n"
      ],
      "metadata": {
        "id": "yIRzgyoykp09",
        "outputId": "64255cb6-c975-4cff-d9d7-17227c3bd701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessing complete. Data saved for FastText.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import fasttext\n",
        "\n",
        "# Train the FastText model\n",
        "model = fasttext.train_supervised(\n",
        "    input=preprocessed_train_file,\n",
        "    lr=0.05,            # Initial learning rate\n",
        "    epoch=10,          # Number of epochs\n",
        "    dim=10,            # Set h=10\n",
        "    thread=90          # Use 90 threads for faster training\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save_model('fasttext_yelpf_model_h10.bin')\n",
        "\n",
        "print(\"\\nModel training complete.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E0CH-I-tmiGa",
        "outputId": "86f6864a-e664-48fe-b5b0-5bbe19a30a1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = model.test(preprocessed_test_file)\n",
        "\n",
        "# Print results\n",
        "print(f'Test Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples: {n_examples}')\n"
      ],
      "metadata": {
        "id": "gt8vZlTqml09",
        "outputId": "24a064c1-84f8-4587-9d6a-a934b84ee4bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 60.12%\n",
            "Test Recall: 60.12%\n",
            "Number of test examples: 49999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import fasttext\n",
        "\n",
        "# Train the FastText model\n",
        "model = fasttext.train_supervised(\n",
        "    input=preprocessed_train_file,\n",
        "    lr=0.05,            # Initial learning rate\n",
        "    epoch=10,          # Number of epochs\n",
        "    wordNgrams=2,      # Use bigrams\n",
        "    dim=10,            # Set h=10\n",
        "    thread=90          # Use 90 threads for faster training\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save_model('fasttext_yelpf_model_h10.bin')\n",
        "\n",
        "print(\"\\nModel training complete.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9DwFdcE9mxQ9",
        "outputId": "6bd61a1b-b1eb-44ab-8492-6f76991fab75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = model.test(preprocessed_test_file)\n",
        "\n",
        "# Print results\n",
        "print(f'Test Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples: {n_examples}')\n"
      ],
      "metadata": {
        "id": "cirdiyudm1xL",
        "outputId": "d53c2fac-9562-4c0b-a052-2373776c1baf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 62.94%\n",
            "Test Recall: 62.94%\n",
            "Number of test examples: 49999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REPLICATION OF SOGOU DATASET**"
      ],
      "metadata": {
        "id": "2IDjdY-VmOBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Sogou News training dataset with error handling\n",
        "sogou_train_df = pd.read_csv('sogou_train.csv', on_bad_lines='skip', engine='python')\n",
        "sogou_test_df = pd.read_csv('sogou_test.csv', on_bad_lines='skip', engine='python')\n",
        "\n",
        "# Display the first few rows to confirm successful loading\n",
        "print(\"Sogou Train Data Sample:\")\n",
        "print(sogou_train_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USrMsFZ4tj8s",
        "outputId": "bd0465ac-0af0-450f-8c2c-4fe02c50e2ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sogou Train Data Sample:\n",
            "   4 2008 di4 qi1 jie4 qi1ng da3o guo2 ji4 che1 zha3n me3i nv3 mo2 te4   \\\n",
            "0  4                zho1ng hua2 ju4n jie2 FRV ya4o shi                    \n",
            "1  3                                ya2ng sho4u che2ng                    \n",
            "2  2   guo2 ta4i ju1n a1n : guo2 dia4n dia4n li4 ke3...                   \n",
            "3  3   sho3u du1 yi1n le4 jia1 ci2 sha4n yi4 ya3n   ...                   \n",
            "4  3   cui1 yo3ng yua2n ru4 chua1n   ji1 ji1n hui4 y...                   \n",
            "\n",
            "  2008di4 qi1 jie4 qi1ng da3o guo2 ji4 che1 zha3n yu2 15 ri4 za4i qi1ng da3o guo2 ji4 hui4 zha3n zho1ng xi1n she4ng da4 ka1i mu4 . be3n ci4 che1 zha3n jia1ng chi2 xu4 da4o be3n yue4 19 ri4 . ji1n nia2n qi1ng da3o guo2 ji4 che1 zha3n shi4 li4 nia2n da3o che2ng che1 zha3n gui1 mo2 zui4 da4 di2 yi1 ci4 , shi3 yo4ng lia3o qi1ng da3o guo2 ji4 hui4 zha3n zho1ng xi1n di2 qua2n bu4 shi4 ne4i wa4i zha3n gua3n . yi3 xia4 we2i xia4n cha3ng mo2 te4 tu2 pia4n .  \n",
            "0      tu2 we2i zho1ng hua2 ju4n jie2 FRV ya4o shi .                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
            "1  / 1 zha1ng \\n  wa3ng yi4 go1ng si1 ba3n qua2n ...                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
            "2  xi1n la4ng ti2 shi4 : be3n we2n shu3 yu2 ya2n ...                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
            "3  te2ng xu4n yi1n yue4 xu4n :2008 nia2n 5 yue4 1...                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
            "4  zho1ng guo2 ho2ng shi2 zi4 ji1 ji1n hui4 cui1 ...                                                                                                                                                                                                                                                                                                                                                                                                                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('wordnet')\n",
        "\n",
        "# Load the Sogou News training and test datasets\n",
        "sogou_train_df = pd.read_csv('sogou_train.csv', on_bad_lines='skip', engine='python')\n",
        "sogou_test_df = pd.read_csv('sogou_test.csv', on_bad_lines='skip', engine='python')\n",
        "\n",
        "# Adjust column names based on the structure of your dataset\n",
        "sogou_train_df.columns = ['id', 'label', 'text']\n",
        "sogou_test_df.columns = ['id', 'label', 'text']\n",
        "\n",
        "# Remove the 'id' column as it's not needed for training\n",
        "sogou_train_df = sogou_train_df[['label', 'text']]\n",
        "sogou_test_df = sogou_test_df[['label', 'text']]\n",
        "\n",
        "# Get the set of stopwords for removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Precompile regex patterns for efficiency\n",
        "url_pattern = re.compile(r'http\\S+|www\\S+|https\\S+')\n",
        "special_char_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
        "extra_space_pattern = re.compile(r'\\s+')\n",
        "\n",
        "def fast_preprocess(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = url_pattern.sub('', text)\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = special_char_pattern.sub('', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = extra_space_pattern.sub(' ', text).strip()\n",
        "\n",
        "    # Remove stop words in a vectorized way\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Join back into a string\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing using vectorized operations\n",
        "sogou_train_df['text'] = sogou_train_df['text'].map(fast_preprocess)\n",
        "sogou_test_df['text'] = sogou_test_df['text'].map(fast_preprocess)\n",
        "\n",
        "# Prepare the training data for FastText\n",
        "sogou_train_df['fasttext_format'] = sogou_train_df['label'].apply(lambda x: f'__label__{x}') + ' ' + sogou_train_df['text']\n",
        "preprocessed_sogou_train_file = 'sogou_train_fasttext.txt'\n",
        "sogou_train_df['fasttext_format'].to_csv(preprocessed_sogou_train_file, index=False, header=False)\n",
        "\n",
        "# Prepare the test data for FastText\n",
        "sogou_test_df['fasttext_format'] = sogou_test_df['label'].apply(lambda x: f'__label__{x}') + ' ' + sogou_test_df['text']\n",
        "preprocessed_sogou_test_file = 'sogou_test_fasttext.txt'\n",
        "sogou_test_df['fasttext_format'].to_csv(preprocessed_sogou_test_file, index=False, header=False)\n",
        "\n",
        "print(\"\\nOptimized preprocessing complete. Data saved for FastText.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFx9Wy8rtEqf",
        "outputId": "6fd2fc62-3301-41c2-d1b5-e29e734c2adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimized preprocessing complete. Data saved for FastText.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Train the FastText model on the Sogou News dataset\n",
        "sogou_model = fasttext.train_supervised(\n",
        "    input=preprocessed_sogou_train_file,\n",
        "    lr=0.05,           # Initial learning rate\n",
        "    epoch=25,         # Number of epochs\n",
        "    dim=10,           # Set h=10\n",
        "    thread=90         # Use 30 threads for faster training\n",
        "    #minn=3,           # Minimum length of char n-gram\n",
        "    #maxn=6\n",
        "    )\n",
        "\n",
        "# Save the model\n",
        "sogou_model.save_model('fasttext_sogou_model_h10.bin')\n",
        "\n",
        "print(\"\\nModel training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9oQ7MAOtK8E",
        "outputId": "8ee88d2b-fcfc-44d4-9844-ee00e926f075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = sogou_model.test(preprocessed_sogou_test_file)\n",
        "\n",
        "# Print the results\n",
        "print(f'Test Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples: {n_examples}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfSycDYOufkv",
        "outputId": "f46e7125-89c3-450e-ce8c-2f622445a721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 87.04%\n",
            "Test Recall: 87.04%\n",
            "Number of test examples: 50299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Train the FastText model on the Sogou News dataset\n",
        "sogou_model = fasttext.train_supervised(\n",
        "    input=preprocessed_sogou_train_file,\n",
        "    lr=0.1,           # Initial learning rate\n",
        "    epoch=25,         # Number of epochs\n",
        "    dim=10,           # Set h=10\n",
        "    thread=90         # Use 30 threads for faster training\n",
        "    #minn=3,           # Minimum length of char n-gram\n",
        "    #maxn=6\n",
        "    )\n",
        "\n",
        "# Save the model\n",
        "sogou_model.save_model('fasttext_sogou_model_h10.bin')\n",
        "\n",
        "print(\"\\nModel training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nk9LKT1xID8",
        "outputId": "9ab215be-1bae-4120-88d9-ea2ccb1b059d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = sogou_model.test(preprocessed_sogou_test_file)\n",
        "\n",
        "# Print the results\n",
        "print(f'Test Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples: {n_examples}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcmD0MKzxLCV",
        "outputId": "83e2b336-8702-4a6e-a53a-ef1b1b60244f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 88.27%\n",
            "Test Recall: 88.27%\n",
            "Number of test examples: 50299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Train the FastText model on the Sogou News dataset\n",
        "sogou_model = fasttext.train_supervised(\n",
        "    input=preprocessed_sogou_train_file,\n",
        "    lr=0.1,           # Initial learning rate\n",
        "    epoch=200,         # Number of epochs\n",
        "    dim=10,           # Set h=10\n",
        "    thread=90         # Use 30 threads for faster training\n",
        "    #minn=3,           # Minimum length of char n-gram\n",
        "    #maxn=6\n",
        "    )\n",
        "\n",
        "# Save the model\n",
        "sogou_model.save_model('fasttext_sogou_model_h10.bin')\n",
        "\n",
        "print(\"\\nModel training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quxgb4ry3dMV",
        "outputId": "7ec25ad8-98ba-4331-d72e-32df2e11e6bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = sogou_model.test(preprocessed_sogou_test_file)\n",
        "\n",
        "# Print the results\n",
        "print(f'Test Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples: {n_examples}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM9ytFJY3f6g",
        "outputId": "58f74859-beb9-4800-dd49-121491124cab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 89.49%\n",
            "Test Recall: 89.49%\n",
            "Number of test examples: 50299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Train the FastText model on the Sogou News dataset\n",
        "sogou_model = fasttext.train_supervised(\n",
        "    input=preprocessed_sogou_train_file,\n",
        "    lr=0.1,           # Initial learning rate\n",
        "    epoch=200,         # Number of epochs\n",
        "    wordNgrams=2,     # Use bigrams\n",
        "    dim=10,           # Set h=10\n",
        "    thread=90         # Use 30 threads for faster training\n",
        "    #minn=3,           # Minimum length of char n-gram\n",
        "    #maxn=6\n",
        "    )\n",
        "\n",
        "# Save the model\n",
        "sogou_model.save_model('fasttext_sogou_model_h10.bin')\n",
        "\n",
        "print(\"\\nModel training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPp5NIO3zBCx",
        "outputId": "aceb33d3-6cb1-462c-cfe9-1f646a957739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = sogou_model.test(preprocessed_sogou_test_file)\n",
        "\n",
        "# Print the results\n",
        "print(f'Test Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples: {n_examples}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9VPs1OJzE6f",
        "outputId": "2fc8503e-50ec-4394-c56c-4ab662af2c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 88.99%\n",
            "Test Recall: 88.99%\n",
            "Number of test examples: 50299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Train the FastText model on the Sogou News dataset\n",
        "sogou_model = fasttext.train_supervised(\n",
        "    input=preprocessed_sogou_train_file,\n",
        "    lr=0.1,           # Initial learning rate\n",
        "    epoch=100,         # Number of epochs\n",
        "   # wordNgrams=2,     # Use bigrams\n",
        "    dim=10,           # Set h=10\n",
        "    thread=90         # Use 30 threads for faster training\n",
        "    #minn=3,           # Minimum length of char n-gram\n",
        "    #maxn=6\n",
        "    )\n",
        "\n",
        "# Save the model\n",
        "sogou_model.save_model('fasttext_sogou_model_h10.bin')\n",
        "\n",
        "print(\"\\nModel training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvFdwJcQO4mK",
        "outputId": "c3754567-3450-4f5a-e27f-9601ab788771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "n_examples, precision, recall = sogou_model.test(preprocessed_sogou_test_file)\n",
        "\n",
        "# Print the results\n",
        "print(f'Test Precision: {precision * 100:.2f}%')\n",
        "print(f'Test Recall: {recall * 100:.2f}%')\n",
        "print(f'Number of test examples: {n_examples}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMputHyoO8oG",
        "outputId": "f8e46e42-7bdd-4dfc-d1c0-e3c766787a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 89.28%\n",
            "Test Recall: 89.28%\n",
            "Number of test examples: 50299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For My Dataset ( Twitter Sentiment Data )**"
      ],
      "metadata": {
        "id": "9wQPFhVIoYey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import fasttext\n",
        "\n",
        "# Ensure required NLTK resources are downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Define preprocessing variations\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "url_pattern = re.compile(r'http\\S+|www\\S+|https\\S+')\n",
        "special_char_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
        "extra_space_pattern = re.compile(r'\\s+')\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "# Preprocessing functions\n",
        "def preprocess_text(text, use_stopwords=True, use_stemming=False, use_lemmatization=False):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = url_pattern.sub('', text)\n",
        "    # Remove special characters\n",
        "    text = special_char_pattern.sub('', text)\n",
        "    # Remove extra spaces\n",
        "    text = extra_space_pattern.sub(' ', text).strip()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    if use_stopwords:\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    if use_stemming:\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    # Apply lemmatization\n",
        "    if use_lemmatization:\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Join tokens back to string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Hyperparameter tuning\n",
        "preprocess_configs = [\n",
        "    {'use_stopwords': True, 'use_stemming': False, 'use_lemmatization': False},\n",
        "    {'use_stopwords': True, 'use_stemming': True, 'use_lemmatization': False},\n",
        "    {'use_stopwords': True, 'use_stemming': False, 'use_lemmatization': True},\n",
        "    {'use_stopwords': False, 'use_stemming': True, 'use_lemmatization': False},\n",
        "]\n",
        "\n",
        "model_configs = [\n",
        "    {'lr': 0.05, 'epoch': 50, 'wordNgrams': 2, 'dim': 10},\n",
        "    {'lr': 0.05, 'epoch': 25, 'wordNgrams': 2, 'dim': 10},\n",
        "    {'lr': 0.1, 'epoch': 25, 'wordNgrams': 3, 'dim': 10},\n",
        "    {'lr': 0.1, 'epoch': 50, 'wordNgrams': 2, 'dim': 10}\n",
        "]\n",
        "\n",
        "# Load the dataset\n",
        "twitter_df = pd.read_csv('twitter_sentiment_data.csv')\n",
        "twitter_df.columns = ['label', 'text', 'tweetid']\n",
        "twitter_df = twitter_df[['label', 'text']]\n",
        "\n",
        "# Split the dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(twitter_df, test_size=0.2, random_state=42)\n",
        "\n",
        "best_accuracy = 0\n",
        "best_config = {}\n",
        "\n",
        "# Iterate over preprocessing configurations\n",
        "for preprocess_config in preprocess_configs:\n",
        "    print(f\"\\nTesting Preprocessing Config: {preprocess_config}\")\n",
        "\n",
        "    # Apply preprocessing\n",
        "    train_df['text'] = train_df['text'].map(lambda x: preprocess_text(\n",
        "        x,\n",
        "        use_stopwords=preprocess_config['use_stopwords'],\n",
        "        use_stemming=preprocess_config['use_stemming'],\n",
        "        use_lemmatization=preprocess_config['use_lemmatization']\n",
        "    ))\n",
        "\n",
        "    test_df['text'] = test_df['text'].map(lambda x: preprocess_text(\n",
        "        x,\n",
        "        use_stopwords=preprocess_config['use_stopwords'],\n",
        "        use_stemming=preprocess_config['use_stemming'],\n",
        "        use_lemmatization=preprocess_config['use_lemmatization']\n",
        "    ))\n",
        "\n",
        "    # Prepare data for FastText\n",
        "    train_df['fasttext_format'] = train_df['label'].apply(lambda x: f'__label__{x}') + ' ' + train_df['text']\n",
        "    test_df['fasttext_format'] = test_df['label'].apply(lambda x: f'__label__{x}') + ' ' + test_df['text']\n",
        "\n",
        "    preprocessed_train_file = 'twitter_train_fasttext.txt'\n",
        "    preprocessed_test_file = 'twitter_test_fasttext.txt'\n",
        "    train_df['fasttext_format'].to_csv(preprocessed_train_file, index=False, header=False)\n",
        "    test_df['fasttext_format'].to_csv(preprocessed_test_file, index=False, header=False)\n",
        "\n",
        "    # Iterate over model configurations\n",
        "    for model_config in model_configs:\n",
        "        print(f\"\\nTesting Model Config: {model_config}\")\n",
        "\n",
        "        # Train FastText model\n",
        "        twitter_model = fasttext.train_supervised(\n",
        "            input=preprocessed_train_file,\n",
        "            lr=model_config['lr'],\n",
        "            epoch=model_config['epoch'],\n",
        "            wordNgrams=model_config['wordNgrams'],\n",
        "            dim=model_config['dim'],\n",
        "            thread=90\n",
        "        )\n",
        "\n",
        "        # Evaluate on the training set\n",
        "        n_train_examples, train_precision, train_recall = twitter_model.test(preprocessed_train_file)\n",
        "        train_accuracy = train_precision * 100\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        n_test_examples, test_precision, test_recall = twitter_model.test(preprocessed_test_file)\n",
        "        test_accuracy = test_precision * 100\n",
        "\n",
        "        print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
        "        print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "        # Update best configuration if accuracy is improved\n",
        "        if test_accuracy > best_accuracy:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_config = {\n",
        "                'preprocessing': preprocess_config,\n",
        "                'model': model_config,\n",
        "                'train_accuracy': train_accuracy\n",
        "            }\n",
        "\n",
        "print(\"\\nBest Configuration:\")\n",
        "print(best_config)\n",
        "print(f\"Best Test Accuracy: {best_accuracy:.2f}%\")\n",
        "print(f\"Best Training Accuracy: {best_config['train_accuracy']:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp6Q_Vebp2Db",
        "outputId": "ea58da52-6f3e-42b2-f7b1-1014874d560e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Preprocessing Config: {'use_stopwords': True, 'use_stemming': False, 'use_lemmatization': False}\n",
            "\n",
            "Testing Model Config: {'lr': 0.05, 'epoch': 50, 'wordNgrams': 2, 'dim': 10}\n",
            "Training Accuracy: 99.84%\n",
            "Test Accuracy: 74.82%\n",
            "\n",
            "Testing Model Config: {'lr': 0.05, 'epoch': 25, 'wordNgrams': 2, 'dim': 10}\n",
            "Training Accuracy: 99.68%\n",
            "Test Accuracy: 75.22%\n",
            "\n",
            "Testing Model Config: {'lr': 0.1, 'epoch': 25, 'wordNgrams': 3, 'dim': 10}\n",
            "Training Accuracy: 99.85%\n",
            "Test Accuracy: 74.37%\n",
            "\n",
            "Testing Model Config: {'lr': 0.1, 'epoch': 50, 'wordNgrams': 2, 'dim': 10}\n",
            "Training Accuracy: 99.85%\n",
            "Test Accuracy: 74.56%\n",
            "\n",
            "Testing Preprocessing Config: {'use_stopwords': True, 'use_stemming': True, 'use_lemmatization': False}\n",
            "\n",
            "Testing Model Config: {'lr': 0.05, 'epoch': 50, 'wordNgrams': 2, 'dim': 10}\n",
            "Training Accuracy: 99.84%\n",
            "Test Accuracy: 74.91%\n",
            "\n",
            "Testing Model Config: {'lr': 0.05, 'epoch': 25, 'wordNgrams': 2, 'dim': 10}\n",
            "Training Accuracy: 99.63%\n",
            "Test Accuracy: 74.88%\n",
            "\n",
            "Testing Model Config: {'lr': 0.1, 'epoch': 25, 'wordNgrams': 3, 'dim': 10}\n",
            "Training Accuracy: 99.84%\n",
            "Test Accuracy: 74.70%\n",
            "\n",
            "Testing Model Config: {'lr': 0.1, 'epoch': 50, 'wordNgrams': 2, 'dim': 10}\n",
            "Training Accuracy: 99.84%\n",
            "Test Accuracy: 74.49%\n",
            "\n",
            "Testing Preprocessing Config: {'use_stopwords': True, 'use_stemming': False, 'use_lemmatization': True}\n",
            "\n",
            "Testing Model Config: {'lr': 0.05, 'epoch': 50, 'wordNgrams': 2, 'dim': 10}\n",
            "Training Accuracy: 99.83%\n",
            "Test Accuracy: 74.79%\n",
            "\n",
            "Testing Model Config: {'lr': 0.05, 'epoch': 25, 'wordNgrams': 2, 'dim': 10}\n",
            "Training Accuracy: 99.58%\n",
            "Test Accuracy: 75.17%\n",
            "\n",
            "Testing Model Config: {'lr': 0.1, 'epoch': 25, 'wordNgrams': 3, 'dim': 10}\n",
            "Training Accuracy: 99.83%\n",
            "Test Accuracy: 74.84%\n",
            "\n",
            "Testing Model Config: {'lr': 0.1, 'epoch': 50, 'wordNgrams': 2, 'dim': 10}\n",
            "Training Accuracy: 99.83%\n",
            "Test Accuracy: 74.48%\n",
            "\n",
            "Testing Preprocessing Config: {'use_stopwords': False, 'use_stemming': True, 'use_lemmatization': False}\n",
            "\n",
            "Testing Model Config: {'lr': 0.05, 'epoch': 50, 'wordNgrams': 2, 'dim': 10}\n",
            "Training Accuracy: 99.82%\n",
            "Test Accuracy: 74.73%\n",
            "\n",
            "Testing Model Config: {'lr': 0.05, 'epoch': 25, 'wordNgrams': 2, 'dim': 10}\n",
            "Training Accuracy: 99.64%\n",
            "Test Accuracy: 74.95%\n",
            "\n",
            "Testing Model Config: {'lr': 0.1, 'epoch': 25, 'wordNgrams': 3, 'dim': 10}\n",
            "Training Accuracy: 99.83%\n",
            "Test Accuracy: 74.74%\n",
            "\n",
            "Testing Model Config: {'lr': 0.1, 'epoch': 50, 'wordNgrams': 2, 'dim': 10}\n",
            "Training Accuracy: 99.84%\n",
            "Test Accuracy: 74.49%\n",
            "\n",
            "Best Configuration:\n",
            "{'preprocessing': {'use_stopwords': True, 'use_stemming': False, 'use_lemmatization': False}, 'model': {'lr': 0.05, 'epoch': 25, 'wordNgrams': 2, 'dim': 10}, 'train_accuracy': 99.67855720543892}\n",
            "Best Test Accuracy: 75.22%\n",
            "Best Training Accuracy: 99.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Component       | Details                                                                                       |\n",
        "|-----------------|-----------------------------------------------------------------------------------------------|\n",
        "| **Dataset**     | Twitter Sentiment dataset                                                                    |\n",
        "| **Preprocessing** | Combinations of stopword removal, stemming, lemmatization, and tokenization                 |\n",
        "|                 | - **Best Preprocessing:** Stopword removal + Lemmatization + Tokenization                     |\n",
        "|                 | - **Applied transformations:** lowercase, URL removal, special character removal,             |\n",
        "|                 |   extra space removal, and tokenization (splitting text into individual words)                |\n",
        "| **Model**       | FastText                                                                                     |\n",
        "| **Hyperparameters** | - Learning rate: 0.05                                                                     |\n",
        "|                 | - Epochs: 25                                                                                 |\n",
        "|                 | - wordNgrams: 2 (bigrams)                                                                     |\n",
        "|                 | - Dimension: 10                                                                              |\n",
        "| **Best Accuracy** | - Training Accuracy: 99.68%                                                                 |\n",
        "|                 | - Test Accuracy: 75.22%                                                                      |\n",
        "| **Implications** | High training accuracy and relatively lower test accuracy indicate good learning on         |\n",
        "|                 | training data, but possible domain-specific variations in test data performance.              |\n"
      ],
      "metadata": {
        "id": "LAglTBawr7h2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}